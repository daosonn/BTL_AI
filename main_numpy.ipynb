{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "tHS8BskwnZa8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-2lJknjKnZbD"
   },
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "l3t_yA3PnZbG",
    "outputId": "2f6fb58f-38a8-4c6a-e38a-c3250dc15e66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (7352, 561)\n",
      "y_train shape: (7352,)\n",
      "x_test shape: (2947, 561)\n",
      "y_test shape: (2947,)\n"
     ]
    }
   ],
   "source": [
    "# Hàm để load dữ liệu từ tệp\n",
    "def load_data(x_path, y_path):\n",
    "    # Load tín hiệu (x) từ file\n",
    "    x_data = np.loadtxt(x_path) \n",
    "    # Load nhãn (y) từ file\n",
    "    y_data = np.loadtxt(y_path, dtype=int) \n",
    "    return x_data, y_data\n",
    "\n",
    "# Đường dẫn đến dữ liệu\n",
    "train_x_path = r\"C:\\Users\\Admin\\OneDrive - Hanoi University of Science and Technology\\Tailieutruong_20241\\AI\\HAR\\UCI HAR Dataset\\UCI HAR Dataset\\train\\X_train.txt\"\n",
    "train_y_path = r\"C:\\Users\\Admin\\OneDrive - Hanoi University of Science and Technology\\Tailieutruong_20241\\AI\\HAR\\UCI HAR Dataset\\UCI HAR Dataset\\train\\y_train.txt\"\n",
    "test_x_path = r\"C:\\Users\\Admin\\OneDrive - Hanoi University of Science and Technology\\Tailieutruong_20241\\AI\\HAR\\UCI HAR Dataset\\UCI HAR Dataset\\test\\X_test.txt\"\n",
    "test_y_path = r\"C:\\Users\\Admin\\OneDrive - Hanoi University of Science and Technology\\Tailieutruong_20241\\AI\\HAR\\UCI HAR Dataset\\UCI HAR Dataset\\test\\y_test.txt\"\n",
    "\n",
    "# Load dữ liệu\n",
    "x_train, y_train = load_data(train_x_path, train_y_path)\n",
    "x_test, y_test = load_data(test_x_path, test_y_path)\n",
    "\n",
    "# Kiểm tra kích thước\n",
    "print(f\"x_train shape: {x_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6VW1sXSinZbJ",
    "outputId": "7d32a53f-60c6-4791-af9e-501808ab8f81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.28858451 -0.02029417 -0.13290514 ... -0.84124676  0.17994061\n",
      "  -0.05862692]\n",
      " [ 0.27841883 -0.01641057 -0.12352019 ... -0.8447876   0.18028889\n",
      "  -0.05431672]\n",
      " [ 0.27965306 -0.01946716 -0.11346169 ... -0.84893347  0.18063731\n",
      "  -0.04911782]\n",
      " ...\n",
      " [ 0.27338737 -0.01701062 -0.04502183 ... -0.77913261  0.24914484\n",
      "   0.04081119]\n",
      " [ 0.28965416 -0.01884304 -0.15828059 ... -0.78518142  0.24643223\n",
      "   0.02533948]\n",
      " [ 0.35150347 -0.01242312 -0.20386717 ... -0.78326693  0.24680852\n",
      "   0.03669484]]\n",
      "[5 5 5 ... 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(x_train)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(y, num_classes):\n",
    "    one_hot = np.zeros((y.size, num_classes))\n",
    "    one_hot[np.arange(y.size), y - 1] = 1  # Adjusting class index to 0-based\n",
    "    return one_hot\n",
    "\n",
    "y_train = one_hot_encode(y_train, num_classes=6)\n",
    "y_test = one_hot_encode(y_test, num_classes=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sjp_WMq4nZbL"
   },
   "source": [
    "### Building Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "tjSI14gvnZbM"
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, sizes, epochs, lr):\n",
    "        self.sizes = sizes\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "\n",
    "        # Initialize weights with Xavier initialization\n",
    "        self.params = {\n",
    "            'W1': np.random.randn(sizes[1], sizes[0]) * np.sqrt(1. / sizes[0]),\n",
    "            'b1': np.zeros((sizes[1], 1)),\n",
    "            'W2': np.random.randn(sizes[2], sizes[1]) * np.sqrt(1. / sizes[1]),\n",
    "            'b2': np.zeros((sizes[2], 1)),\n",
    "            'W3': np.random.randn(sizes[3], sizes[2]) * np.sqrt(1. / sizes[2]),\n",
    "            'b3': np.zeros((sizes[3], 1))\n",
    "        }\n",
    "\n",
    "    def sigmoid(self, x, derivative=False):\n",
    "        if derivative:\n",
    "            return x * (1 - x)  # Efficient derivative\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def softmax(self, x):\n",
    "        exps = np.exp(x - np.max(x, axis=0, keepdims=True))  # Stability trick\n",
    "        return exps / np.sum(exps, axis=0, keepdims=True)\n",
    "\n",
    "    def forward_pass(self, x):\n",
    "        self.params['A0'] = x\n",
    "        self.params['Z1'] = np.dot(self.params['W1'], x) + self.params['b1']\n",
    "        self.params['A1'] = self.sigmoid(self.params['Z1'])\n",
    "        self.params['Z2'] = np.dot(self.params['W2'], self.params['A1']) + self.params['b2']\n",
    "        self.params['A2'] = self.sigmoid(self.params['Z2'])\n",
    "        self.params['Z3'] = np.dot(self.params['W3'], self.params['A2']) + self.params['b3']\n",
    "        self.params['A3'] = self.softmax(self.params['Z3'])\n",
    "        return self.params['A3']\n",
    "\n",
    "    def backward_pass(self, y, output):\n",
    "        grads = {}\n",
    "        error = output - y\n",
    "\n",
    "        grads['W3'] = np.dot(error, self.params['A2'].T)\n",
    "        grads['b3'] = np.sum(error, axis=1, keepdims=True)\n",
    "        error = np.dot(self.params['W3'].T, error) * self.sigmoid(self.params['A2'], derivative=True)\n",
    "\n",
    "        grads['W2'] = np.dot(error, self.params['A1'].T)\n",
    "        grads['b2'] = np.sum(error, axis=1, keepdims=True)\n",
    "        error = np.dot(self.params['W2'].T, error) * self.sigmoid(self.params['A1'], derivative=True)\n",
    "\n",
    "        grads['W1'] = np.dot(error, self.params['A0'].T)\n",
    "        grads['b1'] = np.sum(error, axis=1, keepdims=True)\n",
    "        return grads\n",
    "\n",
    "    def update_parameters(self, grads):\n",
    "        for key in self.params.keys():\n",
    "            if key in grads:\n",
    "                self.params[key] -= self.lr * grads[key]\n",
    "\n",
    "    def compute_loss(self, y, output):\n",
    "        loss = -np.mean(y * np.log(output + 1e-9))  # Cross-entropy\n",
    "        return loss\n",
    "\n",
    "    def train(self, x_train, y_train, x_test, y_test):\n",
    "        for epoch in range(self.epochs):\n",
    "            start_time = time.time()\n",
    "            loss = 0\n",
    "            for i in range(x_train.shape[0]):\n",
    "                x = x_train[i].reshape(-1, 1)\n",
    "                y = y_train[i].reshape(-1, 1)\n",
    "                output = self.forward_pass(x)\n",
    "                loss += self.compute_loss(y, output)\n",
    "                grads = self.backward_pass(y, output)\n",
    "                self.update_parameters(grads)\n",
    "\n",
    "            loss /= x_train.shape[0]\n",
    "            accuracy = self.compute_accuracy(x_test, y_test)\n",
    "            print(f\"Epoch {epoch + 1}/{self.epochs}, Loss: {loss:.4f}, Accuracy: {accuracy:.2f}%, Time: {time.time() - start_time:.2f}s\")\n",
    "\n",
    "    def compute_accuracy(self, x_test, y_test):\n",
    "        correct_predictions = 0\n",
    "        for i in range(x_test.shape[0]):\n",
    "            x = x_test[i].reshape(-1, 1)\n",
    "            y = np.argmax(y_test[i])\n",
    "            output = self.forward_pass(x)\n",
    "            if np.argmax(output) == y:\n",
    "                correct_predictions += 1\n",
    "        return correct_predictions / x_test.shape[0] * 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vWNXkyD_nZbQ"
   },
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "TrdtUa2bnZbS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Loss: 0.1403, Accuracy: 45.78%, Time: 2.96s\n",
      "Epoch 2/30, Loss: 0.0602, Accuracy: 74.48%, Time: 2.98s\n",
      "Epoch 3/30, Loss: 0.0305, Accuracy: 75.33%, Time: 2.86s\n",
      "Epoch 4/30, Loss: 0.0173, Accuracy: 79.13%, Time: 2.97s\n",
      "Epoch 5/30, Loss: 0.0115, Accuracy: 83.37%, Time: 4.95s\n",
      "Epoch 6/30, Loss: 0.0089, Accuracy: 86.56%, Time: 5.18s\n",
      "Epoch 7/30, Loss: 0.0073, Accuracy: 88.36%, Time: 6.24s\n",
      "Epoch 8/30, Loss: 0.0063, Accuracy: 90.06%, Time: 5.11s\n",
      "Epoch 9/30, Loss: 0.0057, Accuracy: 91.01%, Time: 5.11s\n",
      "Epoch 10/30, Loss: 0.0052, Accuracy: 91.92%, Time: 5.65s\n",
      "Epoch 11/30, Loss: 0.0049, Accuracy: 92.40%, Time: 6.07s\n",
      "Epoch 12/30, Loss: 0.0046, Accuracy: 92.91%, Time: 6.41s\n",
      "Epoch 13/30, Loss: 0.0044, Accuracy: 93.11%, Time: 6.79s\n",
      "Epoch 14/30, Loss: 0.0042, Accuracy: 93.18%, Time: 6.30s\n",
      "Epoch 15/30, Loss: 0.0041, Accuracy: 93.32%, Time: 6.62s\n",
      "Epoch 16/30, Loss: 0.0039, Accuracy: 93.38%, Time: 6.32s\n",
      "Epoch 17/30, Loss: 0.0038, Accuracy: 93.28%, Time: 6.68s\n",
      "Epoch 18/30, Loss: 0.0037, Accuracy: 93.25%, Time: 6.46s\n",
      "Epoch 19/30, Loss: 0.0036, Accuracy: 93.11%, Time: 6.61s\n",
      "Epoch 20/30, Loss: 0.0035, Accuracy: 93.18%, Time: 6.51s\n",
      "Epoch 21/30, Loss: 0.0034, Accuracy: 93.55%, Time: 6.74s\n",
      "Epoch 22/30, Loss: 0.0033, Accuracy: 93.82%, Time: 6.75s\n",
      "Epoch 23/30, Loss: 0.0032, Accuracy: 93.76%, Time: 7.06s\n",
      "Epoch 24/30, Loss: 0.0031, Accuracy: 93.79%, Time: 5.30s\n",
      "Epoch 25/30, Loss: 0.0030, Accuracy: 93.82%, Time: 5.09s\n",
      "Epoch 26/30, Loss: 0.0029, Accuracy: 93.86%, Time: 5.12s\n",
      "Epoch 27/30, Loss: 0.0028, Accuracy: 93.79%, Time: 5.12s\n",
      "Epoch 28/30, Loss: 0.0027, Accuracy: 93.48%, Time: 5.86s\n",
      "Epoch 29/30, Loss: 0.0026, Accuracy: 93.48%, Time: 5.76s\n",
      "Epoch 30/30, Loss: 0.0026, Accuracy: 93.59%, Time: 5.58s\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork(sizes=[561, 128, 64, 6], epochs=30, lr=0.01)\n",
    "model.train(x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of W1: (128, 561)\n",
      "Shape of b1: (128, 1)\n",
      "Shape of W2: (64, 128)\n",
      "Shape of b2: (64, 1)\n",
      "Shape of W3: (6, 64)\n",
      "Shape of b3: (6, 1)\n"
     ]
    }
   ],
   "source": [
    "# Lấy trọng số và bias\n",
    "weights_layer1 = model.params['W1']\n",
    "bias_layer1 = model.params['b1']\n",
    "\n",
    "weights_layer2 = model.params['W2']\n",
    "bias_layer2 = model.params['b2']\n",
    "\n",
    "weights_layer3 = model.params['W3']\n",
    "bias_layer3 = model.params['b3']\n",
    "\n",
    "# In kích thước\n",
    "print(\"Shape of W1:\", weights_layer1.shape)\n",
    "print(\"Shape of b1:\", bias_layer1.shape)\n",
    "\n",
    "print(\"Shape of W2:\", weights_layer2.shape)\n",
    "print(\"Shape of b2:\", bias_layer2.shape)\n",
    "\n",
    "print(\"Shape of W3:\", weights_layer3.shape)\n",
    "print(\"Shape of b3:\", bias_layer3.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lưu trọng số và bias \n",
    "np.savetxt(\"weights_layer1.txt\", weights_layer1)\n",
    "np.savetxt(\"bias_layer1.txt\", bias_layer1)\n",
    "\n",
    "np.savetxt(\"weights_layer2.txt\", weights_layer2)\n",
    "np.savetxt(\"bias_layer2.txt\", bias_layer2)\n",
    "\n",
    "np.savetxt(\"weights_layer3.txt\", weights_layer3)\n",
    "np.savetxt(\"bias_layer3.txt\", bias_layer3)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "daoson",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
